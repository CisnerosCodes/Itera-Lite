"""
Optimized SSM kernels with CPU vectorization and custom implementations.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Optional, Tuple
import logging
import time

logger = logging.getLogger(__name__)


class OptimizedSSMKernel:
    """Optimized State Space Model kernel implementations."""
    
    @staticmethod
    def scan_parallel(A: torch.Tensor, B: torch.Tensor, C: torch.Tensor, 
                     D: torch.Tensor, u: torch.Tensor) -> torch.Tensor:
        """
        Parallel scan implementation for SSM.
        More efficient than sequential scan for longer sequences.
        
        Args:
            A: State transition matrix (d_state, d_state)
            B: Input matrix (d_state, d_model)
            C: Output matrix (d_model, d_state)
            D: Skip connection (d_model,)
            u: Input sequence (batch, seq_len, d_model)
            
        Returns:
            Output sequence (batch, seq_len, d_model)
        """
        batch_size, seq_len, d_model = u.shape
        d_state = A.shape[0]
        
        device = u.device
        dtype = u.dtype
        
        # Initialize state
        x = torch.zeros(batch_size, d_state, device=device, dtype=dtype)
        outputs = []
        
        # Vectorized computation
        # Precompute B @ u^T for all timesteps
        Bu = torch.einsum('sd,bld->bls', B, u)  # (batch, seq_len, d_state)
        
        # Sequential scan (can be optimized with parallel scan algorithms)
        for t in range(seq_len):
            # x_{t+1} = A @ x_t + B @ u_t
            x = torch.einsum('ss,bs->bs', A, x) + Bu[:, t, :]
            
            # y_t = C @ x_t + D * u_t
            y = torch.einsum('ds,bs->bd', C, x) + D * u[:, t, :]
            outputs.append(y)
        
        # Stack outputs
        output = torch.stack(outputs, dim=1)  # (batch, seq_len, d_model)
        
        return output
    
    @staticmethod
    def scan_chunked(A: torch.Tensor, B: torch.Tensor, C: torch.Tensor,
                     D: torch.Tensor, u: torch.Tensor, chunk_size: int = 32) -> torch.Tensor:
        """
        Chunked SSM scan for better cache locality.
        
        Args:
            A, B, C, D: SSM parameters
            u: Input sequence
            chunk_size: Size of chunks for processing
            
        Returns:
            Output sequence
        """
        batch_size, seq_len, d_model = u.shape
        d_state = A.shape[0]
        
        device = u.device
        dtype = u.dtype
        
        # Initialize state
        x = torch.zeros(batch_size, d_state, device=device, dtype=dtype)
        outputs = []
        
        # Process in chunks
        for chunk_start in range(0, seq_len, chunk_size):
            chunk_end = min(chunk_start + chunk_size, seq_len)
            u_chunk = u[:, chunk_start:chunk_end, :]
            
            # Precompute for chunk
            Bu_chunk = torch.einsum('sd,bld->bls', B, u_chunk)
            
            chunk_outputs = []
            for t in range(chunk_end - chunk_start):
                x = torch.einsum('ss,bs->bs', A, x) + Bu_chunk[:, t, :]
                y = torch.einsum('ds,bs->bd', C, x) + D * u_chunk[:, t, :]
                chunk_outputs.append(y)
            
            outputs.extend(chunk_outputs)
        
        output = torch.stack(outputs, dim=1)
        return output
    
    @staticmethod
    def scan_optimized(A: torch.Tensor, B: torch.Tensor, C: torch.Tensor,
                       D: torch.Tensor, u: torch.Tensor) -> torch.Tensor:
        """
        Optimized SSM scan using matrix operations where possible.
        
        Args:
            A, B, C, D: SSM parameters
            u: Input sequence (batch, seq_len, d_model)
            
        Returns:
            Output sequence (batch, seq_len, d_model)
        """
        batch_size, seq_len, d_model = u.shape
        d_state = A.shape[0]
        
        device = u.device
        dtype = u.dtype
        
        # For short sequences, use vectorized approach
        if seq_len <= 64:
            # Compute all A^t powers efficiently
            A_powers = [torch.eye(d_state, device=device, dtype=dtype)]
            for _ in range(seq_len - 1):
                A_powers.append(A @ A_powers[-1])
            A_powers = torch.stack(A_powers)  # (seq_len, d_state, d_state)
            
            # Precompute Bu
            Bu = torch.einsum('sd,bld->bls', B, u)  # (batch, seq_len, d_state)
            
            # Compute states for all timesteps
            outputs = []
            for t in range(seq_len):
                # x_t = sum_{i=0}^{t} A^{t-i} @ (B @ u_i)
                x = torch.zeros(batch_size, d_state, device=device, dtype=dtype)
                for i in range(t + 1):
                    x += torch.einsum('ss,bs->bs', A_powers[t - i], Bu[:, i, :])
                
                # y_t = C @ x_t + D * u_t
                y = torch.einsum('ds,bs->bd', C, x) + D * u[:, t, :]
                outputs.append(y)
            
            output = torch.stack(outputs, dim=1)
            return output
        else:
            # For longer sequences, use chunked approach
            return OptimizedSSMKernel.scan_chunked(A, B, C, D, u, chunk_size=32)


class FastSSMLayer(nn.Module):
    """
    SSM layer with optimized kernel implementations.
    Drop-in replacement for standard SSM with performance improvements.
    """
    
    def __init__(self, d_model: int, d_state: int = 64, kernel_type: str = 'optimized'):
        """
        Initialize fast SSM layer.
        
        Args:
            d_model: Model dimension
            d_state: State dimension
            kernel_type: Kernel implementation ('optimized', 'parallel', 'chunked')
        """
        super().__init__()
        
        self.d_model = d_model
        self.d_state = d_state
        self.kernel_type = kernel_type
        
        # SSM parameters
        self.A = nn.Parameter(torch.randn(d_state, d_state) * 0.01)
        self.B = nn.Parameter(torch.randn(d_state, d_model) * 0.01)
        self.C = nn.Parameter(torch.randn(d_model, d_state) * 0.01)
        self.D = nn.Parameter(torch.ones(d_model))
        
        # Select kernel
        if kernel_type == 'optimized':
            self.kernel_fn = OptimizedSSMKernel.scan_optimized
        elif kernel_type == 'parallel':
            self.kernel_fn = OptimizedSSMKernel.scan_parallel
        elif kernel_type == 'chunked':
            self.kernel_fn = lambda A, B, C, D, u: OptimizedSSMKernel.scan_chunked(
                A, B, C, D, u, chunk_size=32
            )
        else:
            raise ValueError(f"Unknown kernel type: {kernel_type}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass with optimized kernel.
        
        Args:
            x: Input tensor (batch, seq_len, d_model)
            
        Returns:
            Output tensor (batch, seq_len, d_model)
        """
        return self.kernel_fn(self.A, self.B, self.C, self.D, x)


def benchmark_ssm_kernels(
    d_model: int = 256,
    d_state: int = 64,
    seq_length: int = 128,
    batch_size: int = 4,
    num_runs: int = 100
) -> dict:
    """
    Benchmark different SSM kernel implementations.
    
    Args:
        d_model: Model dimension
        d_state: State dimension
        seq_length: Sequence length
        batch_size: Batch size
        num_runs: Number of benchmark runs
        
    Returns:
        Benchmark results dictionary
    """
    logger.info("Benchmarking SSM kernels...")
    
    # Create sample input
    x = torch.randn(batch_size, seq_length, d_model)
    
    kernels = ['optimized', 'parallel', 'chunked']
    results = {}
    
    for kernel_type in kernels:
        logger.info(f"  Testing {kernel_type} kernel...")
        
        layer = FastSSMLayer(d_model, d_state, kernel_type=kernel_type)
        layer.eval()
        
        # Warmup
        with torch.no_grad():
            for _ in range(10):
                _ = layer(x)
        
        # Benchmark
        times = []
        with torch.no_grad():
            for _ in range(num_runs):
                start = time.perf_counter()
                output = layer(x)
                times.append(time.perf_counter() - start)
        
        mean_time = np.mean(times) * 1000  # ms
        std_time = np.std(times) * 1000
        
        results[kernel_type] = {
            'mean_latency_ms': mean_time,
            'std_latency_ms': std_time,
            'throughput_samples_per_sec': 1000.0 / mean_time
        }
        
        logger.info(f"    Mean latency: {mean_time:.3f} Â± {std_time:.3f} ms")
    
    # Calculate speedups
    baseline = results['parallel']['mean_latency_ms']
    for kernel_type in kernels:
        speedup = baseline / results[kernel_type]['mean_latency_ms']
        results[kernel_type]['speedup_vs_baseline'] = speedup
        logger.info(f"  {kernel_type}: {speedup:.2f}x vs baseline")
    
    return results


def profile_ssm_operations(d_model: int = 256, d_state: int = 64, 
                          seq_length: int = 128) -> dict:
    """
    Profile individual SSM operations to identify bottlenecks.
    
    Args:
        d_model: Model dimension
        d_state: State dimension  
        seq_length: Sequence length
        
    Returns:
        Profiling results
    """
    import time
    
    logger.info("Profiling SSM operations...")
    
    # Create parameters
    A = torch.randn(d_state, d_state)
    B = torch.randn(d_state, d_model)
    C = torch.randn(d_model, d_state)
    D = torch.ones(d_model)
    u = torch.randn(1, seq_length, d_model)
    x = torch.zeros(1, d_state)
    
    num_runs = 1000
    results = {}
    
    # Profile state transition: A @ x
    times = []
    for _ in range(num_runs):
        start = time.perf_counter()
        _ = torch.einsum('ss,bs->bs', A, x)
        times.append(time.perf_counter() - start)
    results['state_transition_us'] = np.mean(times) * 1e6
    
    # Profile input projection: B @ u
    times = []
    for _ in range(num_runs):
        start = time.perf_counter()
        _ = torch.einsum('sd,bd->bs', B, u[0, 0:1, :])
        times.append(time.perf_counter() - start)
    results['input_projection_us'] = np.mean(times) * 1e6
    
    # Profile output projection: C @ x
    times = []
    for _ in range(num_runs):
        start = time.perf_counter()
        _ = torch.einsum('ds,bs->bd', C, x)
        times.append(time.perf_counter() - start)
    results['output_projection_us'] = np.mean(times) * 1e6
    
    # Profile skip connection: D * u
    times = []
    for _ in range(num_runs):
        start = time.perf_counter()
        _ = D * u[0, 0, :]
        times.append(time.perf_counter() - start)
    results['skip_connection_us'] = np.mean(times) * 1e6
    
    logger.info("Operation timings (microseconds):")
    for op, time_us in results.items():
        logger.info(f"  {op}: {time_us:.2f} Î¼s")
    
    return results


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # Benchmark kernels
    print("\n=== SSM Kernel Benchmark ===")
    results = benchmark_ssm_kernels()
    
    # Profile operations
    print("\n=== SSM Operation Profiling ===")
    profile_results = profile_ssm_operations()
